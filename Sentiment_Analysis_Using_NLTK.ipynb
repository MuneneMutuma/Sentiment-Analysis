{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNck8wNuOvVs9Tx7J5YbzJn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuneneMutuma/Sentiment-Analysis/blob/main/Sentiment_Analysis_Using_NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment Analysis Using NLTK"
      ],
      "metadata": {
        "id": "b03pakD_hOxn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup: Installation and Imports"
      ],
      "metadata": {
        "id": "B7CJkcVHheaA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install nltk"
      ],
      "metadata": {
        "id": "0XrqYBXahV4q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HF11MBvgi5u",
        "outputId": "a5a20b7c-e9f8-486c-bcdf-435d3e732e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.9.11)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.6)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import nltk and other libraries"
      ],
      "metadata": {
        "id": "eJegUVzwhabl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import and Download nltk builtin modules\n",
        "\n"
      ],
      "metadata": {
        "id": "SQwsHw40hl6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('twitter_samples')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4AqWFPViKkq",
        "outputId": "2b72328e-06f3-4850-d5ad-a4b749ab8d52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Import other nltk libraries and helper libraries\n",
        "\n"
      ],
      "metadata": {
        "id": "BPlkQ3vciiCJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
        "from nltk.corpus import twitter_samples, stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "import re, string, random"
      ],
      "metadata": {
        "id": "JE_xLI4PgvG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting Data From Twitter"
      ],
      "metadata": {
        "id": "lcyYX3eQhqB3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this, we are getting positive and negavite tweets. These are already classified. Then we also get sample tweets for tesing."
      ],
      "metadata": {
        "id": "po6mEQofi1Sp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
        "negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
        "text = twitter_samples.strings('tweets.20150430-223406.json')"
      ],
      "metadata": {
        "id": "-OEoZ3QPhuym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normalizing Data and Lemmatization\n",
        "\n",
        "Process of converting data to cannonical form. For example, read, reading, reads, all are from the root \"read\". So it would be well to normalize them to that form."
      ],
      "metadata": {
        "id": "3L2Na85Li0NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_sentence(tokens):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_sentence = []\n",
        "    for word, tag in pos_tag(tokens):\n",
        "        if tag.startswith('NN'):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith('VB'):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
        "\n",
        "    return lemmatized_sentence"
      ],
      "metadata": {
        "id": "zPlHA1hgjEbW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove Noise From Data\n",
        "\n",
        "Noise is any part of text that does not add meaning to it, like commas, semi colons.\n",
        "\n",
        "Also removes stop words, these are words like is, the, a which are very common in a language and would cause an overweight imbalance in prediction yet giving not added help in prediction. So they are also removed."
      ],
      "metadata": {
        "id": "rV0zz2WjjQmJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_noise(tweet_tokens, stop_words = ()):\n",
        "    cleaned_tokens = []\n",
        "\n",
        "    for token, tag in pos_tag(tweet_tokens):\n",
        "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&=#]|[!*\\(\\),]|\\\n",
        "                        (?:%[0-9a-fA-F][0-9a-fA-F]))+', '', token)\n",
        "        token = re.sub(\"(@[A-Za-z0-9_]+)\", \"\", token)\n",
        "\n",
        "        if tag.startswith(\"NN\"):\n",
        "            pos = 'n'\n",
        "        elif tag.startswith(\"VB\"):\n",
        "            pos = 'v'\n",
        "        else:\n",
        "            pos = 'a'\n",
        "\n",
        "        lemmatizer = WordNetLemmatizer()\n",
        "        token = lemmatizer.lemmatize(token, pos)\n",
        "\n",
        "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
        "            cleaned_tokens.append(token.lower())\n",
        "    return cleaned_tokens"
      ],
      "metadata": {
        "id": "D-Um7Q2vjWaN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "\n",
        "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
        "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
        "\n",
        "positive_cleaned_tokens_list = []\n",
        "negative_cleaned_tokens_list = []\n",
        "\n",
        "for tokens in positive_tweet_tokens:\n",
        "    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
        "\n",
        "for tokens in negative_tweet_tokens:\n",
        "    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"
      ],
      "metadata": {
        "id": "2Xpx_mx_kBS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Determine Word Density"
      ],
      "metadata": {
        "id": "7jEnLXuXkXWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_all_words(cleaned_tokens_list):\n",
        "    for tokens in cleaned_tokens_list:\n",
        "        for token in tokens:\n",
        "            yield token\n",
        "\n",
        "all_pos_words = get_all_words(positive_cleaned_tokens_list)"
      ],
      "metadata": {
        "id": "VxGQugPkkZtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "freq_dist_pos = FreqDist(all_pos_words)"
      ],
      "metadata": {
        "id": "QbN2THdMkb2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing Data for Model"
      ],
      "metadata": {
        "id": "cKab132yk8bw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert Tokens to Dictionary"
      ],
      "metadata": {
        "id": "SfZF7YC5k_s5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tweets_for_model(cleaned_tokens_list):\n",
        "    for tweet_tokens in cleaned_tokens_list:\n",
        "        yield dict([token, True] for token in tweet_tokens)\n",
        "\n",
        "poistive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
        "negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"
      ],
      "metadata": {
        "id": "tG0OsI6olC0B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Dataset for Training and Testing Model"
      ],
      "metadata": {
        "id": "pS-RvYCjlFF2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "positive_dataset = [(tweet_dict, \"Positive\")\n",
        "                    for tweet_dict in poistive_tokens_for_model]\n",
        "negative_dataset = [(tweet_dict, \"Negative\")\n",
        "                     for tweet_dict in negative_tokens_for_model]\n",
        "\n",
        "dataset = positive_dataset + negative_dataset\n",
        "\n",
        "random.shuffle(dataset)\n",
        "\n",
        "train_data = dataset[:7000]\n",
        "test_data = dataset[7000:]"
      ],
      "metadata": {
        "id": "jkVK8aFJlLLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Classifier"
      ],
      "metadata": {
        "id": "1bj0Y_c5lM0i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = NaiveBayesClassifier.train(train_data)"
      ],
      "metadata": {
        "id": "rAmLmZzQlSLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n",
        "print(classifier.show_most_informative_features(10))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OzipNrislSft",
        "outputId": "3a96a39a-70f4-4bda-f230-9141bcd4c086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is: 0.9946666666666667\n",
            "Most Informative Features\n",
            "                      :( = True           Negati : Positi =   2041.9 : 1.0\n",
            "                      :) = True           Positi : Negati =   1001.0 : 1.0\n",
            "                follower = True           Positi : Negati =     37.1 : 1.0\n",
            "                  arrive = True           Positi : Negati =     34.4 : 1.0\n",
            "                     sad = True           Negati : Positi =     24.8 : 1.0\n",
            "                     bam = True           Positi : Negati =     22.1 : 1.0\n",
            "                     x15 = True           Negati : Positi =     16.0 : 1.0\n",
            "                      aw = True           Negati : Positi =     14.7 : 1.0\n",
            "               goodnight = True           Positi : Negati =     14.0 : 1.0\n",
            "                followed = True           Negati : Positi =     13.9 : 1.0\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testing"
      ],
      "metadata": {
        "id": "b2FzL5MRlf6H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_sentiment(custom_tweet):\n",
        "    custom_tokens = remove_noise(word_tokenize(custom_tweet))\n",
        "    print(classifier.classify(dict([token, True] for token in custom_tokens)))"
      ],
      "metadata": {
        "id": "qz6EJxpQlh4h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentiment(\"I am very happy today!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJaSnaqDlmOK",
        "outputId": "38e9201b-5e27-4fbb-e7ab-47e5b9bc3774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentiment(\"Broke a sweat there\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FSNGL3A_lr-C",
        "outputId": "e90ac74c-c0d8-4332-bacc-bd7229692066"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentiment(\"That was awfully bad!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lsqnpQXvlvRE",
        "outputId": "e99328df-2ae7-44fa-ed52-dfe787ff4423"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentiment(\"I am not sure about the rain today, but I hope it doesn't rain\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dZm8TSqlyYm",
        "outputId": "0598a5c4-9557-476c-ce48-23643d3f283d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentiment(\"I had a bad score\")"
      ],
      "metadata": {
        "id": "34HY03Mfl4UF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d50e521-6854-4751-faed-3d5e0bceaee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentiment(\"That car aced it\")"
      ],
      "metadata": {
        "id": "4W2xB2uCwMSv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b5a3e4d-bb57-418d-cde2-5962cbdecd0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rsaUKSoY8JuD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}